From 5b3beac3c379c766f332a3f5fc6e8a0026b5a864 Mon Sep 17 00:00:00 2001
From: Alexei Starovoitov <ast@kernel.org>
Date: Sun, 1 Jun 2025 17:31:18 -0700
Subject: [PATCH 4/4] Revert "perf: Fix the throttle logic for a group"

This reverts commit 9734e25fbf5ae68eb04234b2cd14a4b36ab89141.

Signed-off-by: Alexei Starovoitov <ast@kernel.org>
---
 kernel/events/core.c | 66 ++++++++++++++------------------------------
 1 file changed, 20 insertions(+), 46 deletions(-)

diff --git a/kernel/events/core.c b/kernel/events/core.c
index 8327ab0ee641..952340f1df9d 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -2645,39 +2645,6 @@ void perf_event_disable_inatomic(struct perf_event *event)
 static void perf_log_throttle(struct perf_event *event, int enable);
 static void perf_log_itrace_start(struct perf_event *event);
 
-static void perf_event_unthrottle(struct perf_event *event, bool start)
-{
-	event->hw.interrupts = 0;
-	if (start)
-		event->pmu->start(event, 0);
-	perf_log_throttle(event, 1);
-}
-
-static void perf_event_throttle(struct perf_event *event)
-{
-	event->pmu->stop(event, 0);
-	event->hw.interrupts = MAX_INTERRUPTS;
-	perf_log_throttle(event, 0);
-}
-
-static void perf_event_unthrottle_group(struct perf_event *event, bool skip_start_event)
-{
-	struct perf_event *sibling, *leader = event->group_leader;
-
-	perf_event_unthrottle(leader, skip_start_event ? leader != event : true);
-	for_each_sibling_event(sibling, leader)
-		perf_event_unthrottle(sibling, skip_start_event ? sibling != event : true);
-}
-
-static void perf_event_throttle_group(struct perf_event *event)
-{
-	struct perf_event *sibling, *leader = event->group_leader;
-
-	perf_event_throttle(leader);
-	for_each_sibling_event(sibling, leader)
-		perf_event_throttle(sibling);
-}
-
 static int
 event_sched_in(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -2706,8 +2673,10 @@ event_sched_in(struct perf_event *event, struct perf_event_context *ctx)
 	 * ticks already, also for a heavily scheduling task there is little
 	 * guarantee it'll get a tick in a timely manner.
 	 */
-	if (unlikely(event->hw.interrupts == MAX_INTERRUPTS))
-		perf_event_unthrottle(event, false);
+	if (unlikely(event->hw.interrupts == MAX_INTERRUPTS)) {
+		perf_log_throttle(event, 1);
+		event->hw.interrupts = 0;
+	}
 
 	perf_pmu_disable(event->pmu);
 
@@ -4285,8 +4254,12 @@ static void perf_adjust_freq_unthr_events(struct list_head *event_list)
 
 		hwc = &event->hw;
 
-		if (hwc->interrupts == MAX_INTERRUPTS)
-			perf_event_unthrottle_group(event, is_event_in_freq_mode(event));
+		if (hwc->interrupts == MAX_INTERRUPTS) {
+			hwc->interrupts = 0;
+			perf_log_throttle(event, 1);
+			if (!is_event_in_freq_mode(event))
+				event->pmu->start(event, 0);
+		}
 
 		if (!is_event_in_freq_mode(event))
 			continue;
@@ -6208,6 +6181,14 @@ static void __perf_event_period(struct perf_event *event,
 	active = (event->state == PERF_EVENT_STATE_ACTIVE);
 	if (active) {
 		perf_pmu_disable(event->pmu);
+		/*
+		 * We could be throttled; unthrottle now to avoid the tick
+		 * trying to unthrottle while we already re-started the event.
+		 */
+		if (event->hw.interrupts == MAX_INTERRUPTS) {
+			event->hw.interrupts = 0;
+			perf_log_throttle(event, 1);
+		}
 		event->pmu->stop(event, PERF_EF_UPDATE);
 	}
 
@@ -6215,14 +6196,6 @@ static void __perf_event_period(struct perf_event *event,
 
 	if (active) {
 		event->pmu->start(event, PERF_EF_RELOAD);
-		/*
-		 * Once the period is force-reset, the event starts immediately.
-		 * But the event/group could be throttled. Unthrottle the
-		 * event/group now to avoid the next tick trying to unthrottle
-		 * while we already re-started the event/group.
-		 */
-		if (event->hw.interrupts == MAX_INTERRUPTS)
-			perf_event_unthrottle_group(event, true);
 		perf_pmu_enable(event->pmu);
 	}
 }
@@ -10111,7 +10084,8 @@ __perf_event_account_interrupt(struct perf_event *event, int throttle)
 	if (unlikely(throttle && hwc->interrupts >= max_samples_per_tick)) {
 		__this_cpu_inc(perf_throttled_count);
 		tick_dep_set_cpu(smp_processor_id(), TICK_DEP_BIT_PERF_EVENTS);
-		perf_event_throttle_group(event);
+		hwc->interrupts = MAX_INTERRUPTS;
+		perf_log_throttle(event, 0);
 		ret = 1;
 	}
 
-- 
2.49.0

